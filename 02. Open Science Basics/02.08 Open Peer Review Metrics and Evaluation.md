## ![](/Images/Icons/peer_review.png)

## 8. Open Peer Review, Metrics, and Evaluation

### What is it?

To be a researcher is to find oneself under constant evaluation. Academia is a "prestige economy", where an academic's worth is very often valued in evaluations of the levels of esteem within which they are held by their peers, decision-makers and others \(Blackmore and Kandiko, 2011\). This esteem is manifested in many methods of research assessment.

Peer review is the formal quality assurance mechanism whereby scholarly manuscripts \(e.g., journal articles, books, grant applications and conference papers\) are made subject to the scrutiny of others, whose feedback and judgements are then used to improve works and make final decisions regarding selection \(for publication, grant allocation or speaking time\). Peer review presents researchers with opportunities for engaging with novel research, building academic networks and expertise, and refining their own writing skills. It is a crucial element of quality control for academic work. Yet, in general, researchers do not often receive formal training in how to do peer review. Even where researchers believe themselves confident with traditional peer review, however, the many forms of open peer review present new challenges and opportunities. Open peer review \(OPR\) aims to bring greater transparency and participation to formal and informal peer review processes.

Moreover, current rewards and evaluation in science and scholarship are not \(yet\) in line with Open Science. The metrics used to evaluate research \(e.g. Journal Impact Factor, h-index\) do not measure - and therefore do not reward - open research practices. Open peer review activity is not necessarily recognized as "scholarship" in professional advancement scenarios \(e.g. in many cases, grant reviewers don’t consider even the most brilliant open peer reviews to be scholarly objects unto themselves\). Furthermore, many evaluation metrics - especially certain types of bibliometrics - are not as open and transparent as the community would like.

Under those circumstances, at best Open Science practices are seen as an additional burden without rewards. At worst, they are seen as actively damaging chances of future funding and promotion & tenure.

### Rationale

Open Peer Review means different things to different people and communities and has been defined as "an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science" \(Ross-Hellauer, 2017\). Its two main traits are “open identities”, where both authors and reviewers are aware of each other’s identities \(i.e., non-blinded\), and “open reports”, where review reports are published alongside the relevant article. These traits can be combined, but need not be, and may be complemented by other innovations, such as “open participation”, where members of the wider community are able to contribute to the review process, “open interaction”, where direct reciprocal discussion between author\(s\) and reviewers, and/or between reviewers, is allowed and encouraged, and “open pre-review manuscripts”, where manuscripts are made immediately available in advance of any formal peer review procedures \(either internally as part of journal workflows or externally via preprint servers\). As OPR covers such a diverse range of practices, there are many considerations for reviewers and authors to take into account.

Regarding evaluation, a recent report from the European Commission recognizes that that there are basically two approaches to Open Science implementation and the way rewards and evaluation can support that:

1. Simply support the status quo by encouraging more openness, building related metrics and quantifying outputs;

2. Experiment with alternative research practices and assessment, open data, citizen science and open education.

More and more funders and institutions are taking steps in these directions, for example by moving away from simple counts, and including narratives \("what are your 5 most important papers, and why?"\) \[ref REF - check!\] and indications of societal impact. Other steps funders are taking are allowing more types of research output \(such as preprints\) in applications \[ref NIH\] and funding different types of research \(such as replication studies\) \[ref NWO\]

The [San Francisco Declaration on Research Assessment \(DORA\)](https://sfdora.org/) recommends moving away from journal based evaluations, consider all types of output and use various forms of metrics and narrative assessment in parallel. DORA has been signed by thousands of researchers, institutions, publishers and funders, who have now committed themselves to putting this in practice. The [Leiden Manifesto](http://www.leidenmanifesto.org/) provides guidance on how to use metrics responsibly.



![](/Images/02 Open Science Basics/02_open_peer_review.png)

### Learning objectives

OPR, in its different forms, has many potential advantages for reviewers and authors:

* Open identities \(non-blinded\) review fosters greater accountability amongst reviewers and reduces the opportunities for bias or undisclosed conflicts of interest.

* Open review reports add another layer of quality assurance, allowing the wider community to scrutinize reviews to examine decision-making processes.

* In combination, open identities and open reports are theorized to lead to better reviews, as the thought of having their name publicly connected to a work or seeing their review published encourages reviewers to be more thorough.

* Open identities and open reports enable reviewers to gain public credit for their review work, thus incentivising this vital activity and allowing review work to be cited in other publications and in career development activities linked to promotion and tenure.

* Open participation could overcome problems associated with editorial selection of reviewers \(e.g., biases, closed-networks, elitism\). Especially for early career researchers who do not yet receive invitations to review, such open processes may also present a chance to build their research reputation and practice their review skills.

Another way to promote Open Science is by making its direct incentives to researchers more visible: widening availability of research, increasing collaboration, increasing \(societal\) participation, speeding up insights and innovation, reducing waste, detecting errors earlier, improving the trust in and image of science and scholarship.

### Key components

#### 

### Knowledge

Mention any existing platforms for open peer review \(such as [https://publons.com](https://publons.com)\).

Platforms, methods and tools of open peer review. OpenUPhub: [https://www.openuphub.eu/review/must-reads](https://www.openuphub.eu/review/must-reads)

**Open metrics**

\[BK\]

* metrics vs. more qualitative assessment

* responsible use of metrics

* altmetrics

* metrics for measuring openness \(look at RAND dashboard for EC Open Science initiative; see if there are other "openness" metrics beyond Impactstory’s openness badge\)

* openness of metrics

  * What makes a metric "open": transparency vs proprietary, intentional opacity vs. difficulty of communicating nuance

#### 

### Skills

Example exercises

* Trainees work in groups of three. Each individually writes a review of a short academic text

* Review a paper on a pre-print server

* Use a free bibliometrics or altmetrics service \(e.g. Impactstory, Paperbuzz, Altmetric bookmarklet, Dimensions.ai\) to look up metrics for a paper, then write a short explanation of how exactly various metrics reported by each service are calculated \(it’s harder than you’d assume; this would get at the challenges of finding proper metrics documentation for even the seemingly most transparent services\)

### 

### Questions, obstacles, and common misconceptions

### However, there are potential disadvantages:

* Open identities removes anonymity conditions for reviewers \(single-blind\) or authors and reviewers \(double-blind\) which are traditionally in place to counteract social biases \(although there is not strong-evidence that such anonymity has been effective\). It’s therefore important for reviewers to constantly question their assumptions to ensure their judgements reflect only the quality of the manuscript, and not the status, history, or affiliations of the author\(s\). Authors should do the same in receiving peer review comments.

* Giving and receiving criticism is often a process fraught with unavoidably emotional reactions - authors and reviewers may subjectively agree or disagree on how to present the results and/or what needs improvement, amendment or correction. In open identities and/or open reports, the transparency could exacerbate such difficulties. It is therefore essential that reviewers ensure that they communicate their points in a clear and civil way, in order to maximise the chances that it will be received as valuable feedback by the author\(s\).

* Lack of anonymity for reviewers in open identities review might subvert the process by discouraging reviewers from making strong criticisms, especially against higher-status colleagues.

* Finally, given these issues, potential reviewers may be more likely to decline to review.

### 

### Learning outcomes

### 

### Further reading

* Peer Review the Nuts and Bolts. A Guide for Early Career Researchers \([http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf](http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf)\)

* Peer Reviewers’ Openness Initiative \([https://opennessinitiative.org/](https://opennessinitiative.org/)\)

* Open Rev. \([https://www.openrev.org/](https://www.openrev.org/)\)

* Peerage of Science \([https://www.peerageofscience.org/](https://www.peerageofscience.org/)\)

May be of interest \(here or in the Practical guidance chapter\):

* Make Data Count \([https://makedatacount.org/](https://makedatacount.org/)\)

* Leiden Manifesto for Research Metrics \([http://www.leidenmanifesto.org/](http://www.leidenmanifesto.org/)\)

* Responsible Metrics \([https://responsiblemetrics.org/](https://responsiblemetrics.org/)\)

* NISO Alternative Assessment Metrics \(Altmetrics\) Initiative \([http://www.niso.org/standards-committees/altmetrics](http://www.niso.org/standards-committees/altmetrics)\)

* Snowball Metrics \([https://www.snowballmetrics.com/](https://www.snowballmetrics.com/)\)

[https://ec.europa.eu/research/openscience/pdf/os\_rewards\_wgreport\_final.pdf](https://ec.europa.eu/research/openscience/pdf/os_rewards_wgreport_final.pdf) - integrate this.

